{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "path = \"../data/dl1/dogscats/\"\n",
    "# path = \"../data/dl1/dogscats/sample/\"\n",
    "#TYPE = 'train'\n",
    "TYPE = 'valid'\n",
    "SEED = 1101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division,print_function\n",
    "import os, json\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, linewidth=100)\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.spatial.distance\n",
    "import utils; reload(utils)\n",
    "from utils import plots\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import vgg class\n",
    "import vgg16; reload(vgg16)\n",
    "from vgg16 import Vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define submodels to extract representations at a given layer\n",
    "from keras.models import Model\n",
    "idx = [0,5,10,17,24,31,33,35,37]\n",
    "\n",
    "# lambda_1\n",
    "# maxpooling2d_1\n",
    "# maxpooling2d_2\n",
    "# maxpooling2d_3\n",
    "# maxpooling2d_4\n",
    "# maxpooling2d_5\n",
    "# dense_1\n",
    "# dense_2\n",
    "# dense_3\n",
    "\n",
    "def get_models(idx,base):\n",
    "    layers  = []\n",
    "    for i in range(len(idx)):\n",
    "        print(base.layers[idx[i]].name)\n",
    "        layers.append( base.layers[idx[i]] )\n",
    "\n",
    "    models = []\n",
    "    outshapes = []\n",
    "    for layer in layers:\n",
    "        models.append( Model(input=base.input,\n",
    "                             output=base.get_layer(layer.name).output) )\n",
    "        outshapes.append(list( layer.output_shape ))\n",
    "        \n",
    "        \n",
    "    print('Done.')\n",
    "    return layers,models,outshapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda_1\n",
      "maxpooling2d_1\n",
      "maxpooling2d_2\n",
      "maxpooling2d_3\n",
      "maxpooling2d_4\n",
      "maxpooling2d_5\n",
      "dense_1\n",
      "dense_2\n",
      "dense_3\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# instantiate base model and sub-models\n",
    "vgg = Vgg16()\n",
    "base = vgg.model\n",
    "layers,models,outshapes = get_models(idx,base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract representations without finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N.of batches 40\n"
     ]
    }
   ],
   "source": [
    "# I extract the representations of 2000 images in the training or validation set (TYPE)\n",
    "datapath = path + TYPE + '/'\n",
    "repath = '../data/dl1/objnc/rep/dogscats/nofinetune/'\n",
    "batch_size = 50\n",
    "nbatches   = 40\n",
    "print('N.of batches ' + str(nbatches))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# get labels for future use\n",
    "batches = vgg.get_batches(datapath, batch_size=batch_size, shuffle=True, seed=SEED)\n",
    "L = []\n",
    "for n in range(nbatches):\n",
    "    _,labels = batches.next()      \n",
    "    L.append(labels)\n",
    "L = np.asarray(L, dtype=float) \n",
    "L.shape = (nbatches*batch_size,2)\n",
    "np.save(repath +  'labels' + '_' + TYPE, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing model : lambda_1\n",
      "Found 2000 images belonging to 2 classes.\n",
      "(2000, 3, 224, 224)\n",
      "408.567681074\n",
      "Processing model : maxpooling2d_1\n",
      "Found 2000 images belonging to 2 classes.\n",
      "(2000, 64, 112, 112)\n",
      "2129.75369287\n",
      "Processing model : maxpooling2d_2\n",
      "Found 2000 images belonging to 2 classes.\n",
      "(2000, 128, 56, 56)\n",
      "1053.20662999\n",
      "Processing model : maxpooling2d_3\n",
      "Found 2000 images belonging to 2 classes.\n",
      "(2000, 256, 28, 28)\n",
      "563.425999165\n",
      "Processing model : maxpooling2d_4\n",
      "Found 2000 images belonging to 2 classes.\n",
      "(2000, 512, 14, 14)\n",
      "317.03707099\n",
      "Processing model : maxpooling2d_5\n",
      "Found 2000 images belonging to 2 classes.\n",
      "(2000, 512, 7, 7)\n",
      "134.26051712\n",
      "Processing model : dense_1\n",
      "Found 2000 images belonging to 2 classes.\n",
      "(2000, 4096)\n",
      "83.6617281437\n",
      "Processing model : dense_2\n",
      "Found 2000 images belonging to 2 classes.\n",
      "(2000, 4096)\n",
      "84.3319208622\n",
      "Processing model : dense_3\n",
      "Found 2000 images belonging to 2 classes.\n",
      "(2000, 1000)\n",
      "77.0289649963\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# For each batch extract representations. Notice that in case TYPE is 'train' you need to\n",
    "# set the SEED in get_batches to enforce reproducibility. \n",
    "# This is not needed when TYPE is 'valid' since there we use the whole validation data but we use it\n",
    "# anyway. In this way we can separately capture the images and correct labels for future use, using\n",
    "# the same batch_size\n",
    "\n",
    "D = []\n",
    "D_sq = []\n",
    "for layer,model in zip(layers,models):\n",
    "    print('Processing model : ' + layer.name)\n",
    "    R = []\n",
    "    batches = vgg.get_batches(datapath, batch_size=batch_size, shuffle=True, seed=SEED)\n",
    "    ti = time()\n",
    "    for n in range(nbatches):       \n",
    "        imgs,labels = batches.next()       \n",
    "        R.append(model.predict(imgs))\n",
    "    R = np.asarray(R, dtype=float) \n",
    "    outshape = list(layer.output_shape)\n",
    "    outshape[0] = nbatches*batch_size\n",
    "    R.shape = tuple(outshape)\n",
    "    print(R.shape)\n",
    "    np.save(repath +  layer.name + '_' + TYPE, R)\n",
    "    \n",
    "    # compute distances\n",
    "    R.shape = R.shape[0], -1\n",
    "    d = scipy.spatial.distance.pdist(R, 'euclidean')\n",
    "    d_sq = scipy.spatial.distance.squareform(d, force='no', checks=True)\n",
    "    D.append(d)\n",
    "    D_sq.append(d_sq)\n",
    "    te = time() - ti\n",
    "    print(te) \n",
    "    \n",
    "np.save(repath + 'D'    + '_' + TYPE, D)\n",
    "np.save(repath + 'D_sq' + '_' + TYPE, D_sq)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# define batches, now this is not constrained to be 50 anymore. We use these batches for training only\n",
    "batch_size = 64\n",
    "batches = vgg.get_batches(path+'train', batch_size=batch_size, seed=SEED)\n",
    "val_batches = vgg.get_batches(path+'valid', batch_size=batch_size, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg.finetune(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "23000/23000 [==============================] - 650s - loss: 0.1188 - acc: 0.9697 - val_loss: 0.0795 - val_acc: 0.9785\n",
      "Epoch 2/5\n",
      "23000/23000 [==============================] - 651s - loss: 0.0966 - acc: 0.9772 - val_loss: 0.0674 - val_acc: 0.9860\n",
      "Epoch 3/5\n",
      "23000/23000 [==============================] - 652s - loss: 0.1048 - acc: 0.9774 - val_loss: 0.0875 - val_acc: 0.9810\n",
      "Epoch 4/5\n",
      "23000/23000 [==============================] - 652s - loss: 0.1025 - acc: 0.9789 - val_loss: 0.0807 - val_acc: 0.9850\n",
      "Epoch 5/5\n",
      "23000/23000 [==============================] - 652s - loss: 0.1021 - acc: 0.9801 - val_loss: 0.0786 - val_acc: 0.9850\n"
     ]
    }
   ],
   "source": [
    "vgg.fit(batches, val_batches, nb_epoch=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observation \n",
    "I notice that it's doing strange things but this is not important since we need just to compute anew the representations of the last hidden layer - the new one inserted to substitute the last hidden layer in the original VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "repath   = '../data/dl1/objnc/rep/dogscats/finetune/'\n",
    "vgg.model.save_weights(repath + 'w_ft.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract representations last hidden layer finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda_1\n",
      "maxpooling2d_1\n",
      "maxpooling2d_2\n",
      "maxpooling2d_3\n",
      "maxpooling2d_4\n",
      "maxpooling2d_5\n",
      "dense_1\n",
      "dense_2\n",
      "dense_4\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "base = vgg.model\n",
    "layers,models,outshapes = get_models(idx,base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "N.of batches 40\n"
     ]
    }
   ],
   "source": [
    "datapath = path + TYPE + '/'\n",
    "batch_size = 50\n",
    "nbatches   = 40\n",
    "batches = vgg.get_batches(datapath, batch_size=batch_size, shuffle=True, seed = SEED)\n",
    "print('N.of batches ' + str(nbatches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing model : dense_4\n",
      "(2000, 2)\n",
      "74.9427008629\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "repath = '../data/dl1/objnc/rep/dogscats/finetune/'\n",
    "\n",
    "D = []\n",
    "D_sq = []\n",
    "layer = layers[-1]\n",
    "model = models[-1]\n",
    "print('Processing model : ' + layer.name)\n",
    "\n",
    "R = []\n",
    "ti = time()\n",
    "for n in range(nbatches):\n",
    "    imgs,_ = batches.next()       \n",
    "    R.append(model.predict(imgs))\n",
    "R = np.asarray(R, dtype=float) \n",
    "outshape = list(layer.output_shape)\n",
    "outshape[0] = nbatches*batch_size\n",
    "R.shape = tuple(outshape)\n",
    "print(R.shape)\n",
    "np.save(repath +  layer.name + '_' + TYPE, R)\n",
    "\n",
    "# compute distances\n",
    "R.shape = R.shape[0], -1\n",
    "d = scipy.spatial.distance.pdist(R, 'euclidean')\n",
    "d_sq = scipy.spatial.distance.squareform(d, force='no', checks=True)\n",
    "D.append(d)\n",
    "D_sq.append(d_sq)\n",
    "te = time() - ti\n",
    "print(te) \n",
    "    \n",
    "np.save(repath + 'D'    + '_' + TYPE, D)\n",
    "np.save(repath + 'D_sq' + '_' + TYPE, D_sq)\n",
    "print('Done.')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
