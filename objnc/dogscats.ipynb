{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "path = \"../data/dl1/dogscats/\"\n",
    "# path = \"../data/dl1/dogscats/sample/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division,print_function\n",
    "\n",
    "import os, json\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, linewidth=100)\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.spatial.distance\n",
    "\n",
    "import utils; reload(utils)\n",
    "from utils import plots\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import our class, and instantiate\n",
    "import vgg16; reload(vgg16)\n",
    "from vgg16 import Vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "idx = [0,5,10,17,24,31,33,35,37]\n",
    "# lambda_1\n",
    "# maxpooling2d_1\n",
    "# maxpooling2d_2\n",
    "# maxpooling2d_3\n",
    "# maxpooling2d_4\n",
    "# maxpooling2d_5\n",
    "# dense_1\n",
    "# dense_2\n",
    "# dense_3\n",
    "\n",
    "def get_models(idx,base):\n",
    "    layers  = []\n",
    "    for i in range(len(idx)):\n",
    "        print(base.layers[idx[i]].name)\n",
    "        layers.append( base.layers[idx[i]] )\n",
    "\n",
    "    models = []\n",
    "    outshapes = []\n",
    "    for layer in layers:\n",
    "        models.append( Model(input=base.input,\n",
    "                             output=base.get_layer(layer.name).output) )\n",
    "        outshapes.append(list( layer.output_shape ))\n",
    "        \n",
    "        \n",
    "    print('Done.')\n",
    "    return layers,models,outshapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda_1\n",
      "maxpooling2d_1\n",
      "maxpooling2d_2\n",
      "maxpooling2d_3\n",
      "maxpooling2d_4\n",
      "maxpooling2d_5\n",
      "dense_1\n",
      "dense_2\n",
      "dense_3\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "vgg = Vgg16()\n",
    "base = vgg.model\n",
    "layers,models,outshapes = get_models(idx,base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract representations without finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "N.of batches 40\n",
      "Processing model : lambda_1\n",
      "Found 23000 images belonging to 2 classes.\n",
      "(2000, 3, 224, 224)\n",
      "395.020223856\n",
      "Processing model : maxpooling2d_1\n",
      "Found 23000 images belonging to 2 classes.\n",
      "(2000, 64, 112, 112)\n"
     ]
    }
   ],
   "source": [
    "# I extract the representations for the training set and the validation set separately\n",
    "datapath = path + 'train/'\n",
    "repath   = '../data/dl1/objnc/rep/dogscats/nofinetune/'\n",
    "nbatches = 40\n",
    "batch_size=50\n",
    "batches = vgg.get_batches(datapath, batch_size=batch_size)\n",
    "print('N.of batches ' + str(nbatches))\n",
    "\n",
    "\n",
    "D = []\n",
    "D_sq = []\n",
    "for layer,model in zip(layers,models):\n",
    "    print('Processing model : ' + layer.name)\n",
    "    R = []\n",
    "    batches = vgg.get_batches(datapath, batch_size=batch_size, shuffle=False)\n",
    "    ti = time()\n",
    "    #for n in range(nbatches):\n",
    "    for n in range(nbatches):\n",
    "        imgs,_ = batches.next()       \n",
    "        R.append(model.predict(imgs))\n",
    "    R = np.asarray(R, dtype=float) \n",
    "    outshape = list(layer.output_shape)\n",
    "    outshape[0] = nbatches*batch_size\n",
    "    R.shape = tuple(outshape)\n",
    "    print(R.shape)\n",
    "    np.save(repath +  layer.name + '_train', R)\n",
    "    \n",
    "    # compute distances\n",
    "    R.shape = R.shape[0], -1\n",
    "    d = scipy.spatial.distance.pdist(R, 'euclidean')\n",
    "    d_sq = scipy.spatial.distance.squareform(d, force='no', checks=True)\n",
    "    D.append(d)\n",
    "    D_sq.append(d_sq)\n",
    "    te = time() - ti\n",
    "    print(te) \n",
    "    \n",
    "np.save(repath + 'D'    + '_train', D)\n",
    "np.save(repath + 'D_sq' + '_train', D_sq)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract representations with finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "vgg = Vgg16()\n",
    "N = 23000\n",
    "batch_size = 50\n",
    "nbatches = int(N/batch_size)\n",
    "batches = vgg.get_batches(path+'train', batch_size=batch_size)\n",
    "val_batches = vgg.get_batches(path+'valid', batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg.finetune(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "23000/23000 [==============================] - 600s - loss: 0.1233 - acc: 0.9683 - val_loss: 0.0549 - val_acc: 0.9845\n",
      "Epoch 2/10\n",
      "23000/23000 [==============================] - 601s - loss: 0.1004 - acc: 0.9777 - val_loss: 0.0728 - val_acc: 0.9845\n",
      "Epoch 3/10\n",
      "23000/23000 [==============================] - 601s - loss: 0.1070 - acc: 0.9780 - val_loss: 0.0719 - val_acc: 0.9830\n",
      "Epoch 4/10\n",
      "23000/23000 [==============================] - 601s - loss: 0.1094 - acc: 0.9790 - val_loss: 0.0824 - val_acc: 0.9840\n",
      "Epoch 5/10\n",
      "23000/23000 [==============================] - 602s - loss: 0.1098 - acc: 0.9801 - val_loss: 0.0721 - val_acc: 0.9880\n",
      "Epoch 6/10\n",
      "23000/23000 [==============================] - 601s - loss: 0.1184 - acc: 0.9793 - val_loss: 0.0756 - val_acc: 0.9835\n",
      "Epoch 7/10\n",
      "23000/23000 [==============================] - 601s - loss: 0.1188 - acc: 0.9807 - val_loss: 0.0664 - val_acc: 0.9845\n",
      "Epoch 8/10\n",
      "23000/23000 [==============================] - 600s - loss: 0.1215 - acc: 0.9812 - val_loss: 0.0924 - val_acc: 0.9830\n",
      "Epoch 9/10\n",
      "23000/23000 [==============================] - 600s - loss: 0.1275 - acc: 0.9810 - val_loss: 0.1014 - val_acc: 0.9835\n",
      "Epoch 10/10\n",
      "23000/23000 [==============================] - 601s - loss: 0.1185 - acc: 0.9817 - val_loss: 0.0890 - val_acc: 0.9855\n"
     ]
    }
   ],
   "source": [
    "vgg.fit(batches, val_batches, nb_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observation \n",
    "I notice that it's doing strange things but this is not important since we need just to compute anew the representations of the last hidden layer - the new one inserted to substitute the last hidden layer in the original VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "repath   = '../data/dl1/objnc/rep/dogscats/finetune/'\n",
    "vgg.model.save_weights(repath + 'w.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract representations last hidden layer finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda_4\n",
      "maxpooling2d_16\n",
      "maxpooling2d_17\n",
      "maxpooling2d_18\n",
      "maxpooling2d_19\n",
      "maxpooling2d_20\n",
      "dense_10\n",
      "dense_11\n",
      "dense_13\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "base = vgg.model\n",
    "layers,models,outshapes = get_models(idx,base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x7fe01d4bb710>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "N.of batches 40\n",
      "Processing model : dense_13\n",
      "Found 23000 images belonging to 2 classes.\n",
      "(2000, 2)\n",
      "70.2854681015\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "datapath = path + 'train/'\n",
    "repath   = '../data/dl1/objnc/rep/dogscats/finetune/'\n",
    "nbatches = 40\n",
    "batch_size=50\n",
    "batches = vgg.get_batches(datapath, batch_size=batch_size)\n",
    "print('N.of batches ' + str(nbatches))\n",
    "\n",
    "D = []\n",
    "D_sq = []\n",
    "layer = layers[-1]\n",
    "model = models[-1]\n",
    "print('Processing model : ' + layer.name)\n",
    "\n",
    "R = []\n",
    "batches = vgg.get_batches(datapath, batch_size=batch_size, shuffle=False)\n",
    "ti = time()\n",
    "for n in range(nbatches):\n",
    "    imgs,_ = batches.next()       \n",
    "    R.append(model.predict(imgs))\n",
    "R = np.asarray(R, dtype=float) \n",
    "outshape = list(layer.output_shape)\n",
    "outshape[0] = nbatches*batch_size\n",
    "R.shape = tuple(outshape)\n",
    "print(R.shape)\n",
    "np.save(repath +  layer.name + '_train', R)\n",
    "\n",
    "# compute distances\n",
    "R.shape = R.shape[0], -1\n",
    "d = scipy.spatial.distance.pdist(R, 'euclidean')\n",
    "d_sq = scipy.spatial.distance.squareform(d, force='no', checks=True)\n",
    "D.append(d)\n",
    "D_sq.append(d_sq)\n",
    "te = time() - ti\n",
    "print(te) \n",
    "    \n",
    "np.save(repath + 'D'    + '_train', D)\n",
    "np.save(repath + 'D_sq' + '_train', D_sq)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
